{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11294574,"sourceType":"datasetVersion","datasetId":7062300}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install torch torchvision pillow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T16:09:36.336137Z","iopub.execute_input":"2025-04-07T16:09:36.336399Z","iopub.status.idle":"2025-04-07T16:09:39.856184Z","shell.execute_reply.started":"2025-04-07T16:09:36.336364Z","shell.execute_reply":"2025-04-07T16:09:39.855192Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport gc\n\ndef clear_gpu_memory():\n    torch.cuda.empty_cache()\n    gc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T16:09:39.857183Z","iopub.execute_input":"2025-04-07T16:09:39.857682Z","iopub.status.idle":"2025-04-07T16:09:41.486113Z","shell.execute_reply.started":"2025-04-07T16:09:39.857647Z","shell.execute_reply":"2025-04-07T16:09:41.485389Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\nfrom torchvision import datasets, transforms\nfrom PIL import Image\nimport os\n\n# Mount Drive (you’ve probably already done this step)\n# from google.colab import drive\n# drive.mount('/content/drive')\n\n# --- Path to your dataset inside Google Drive ---\ndata_root = '/kaggle/input/fashion-gan/dataset_new_1500'\n\n# --- Define custom dataset for pairing ---\nclass PairedDataset(Dataset):\n    def __init__(self, sketch_dataset, real_dataset):\n        self.sketch_dataset = sketch_dataset\n        self.real_dataset = real_dataset\n\n    def __len__(self):\n        return min(len(self.sketch_dataset), len(self.real_dataset))\n\n    def __getitem__(self, idx):\n        sketch_img, _ = self.sketch_dataset[idx]\n        real_img, _ = self.real_dataset[idx]\n        return sketch_img, real_img\n\n# --- Define transforms ---\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(256),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# --- Load training datasets ---\ntrain_sketch = datasets.ImageFolder(root=os.path.join(data_root, 'train/train_sketch_1500/'), transform=transform)\ntrain_real = datasets.ImageFolder(root=os.path.join(data_root, 'train/train_real_1500/'), transform=transform)\n\ntrain_dataset = PairedDataset(train_sketch, train_real)\ntrain_loader = DataLoader(train_dataset, batch_size=7, shuffle=True)\n\n# --- Load validation datasets ---\nval_sketch = datasets.ImageFolder(root=os.path.join(data_root, 'valid/valid_sketch_1500/'), transform=transform)\nval_real = datasets.ImageFolder(root=os.path.join(data_root, 'valid/valid_real_1500/'), transform=transform)\n\nval_dataset = PairedDataset(val_sketch, val_real)\nval_loader = DataLoader(val_dataset, batch_size=7, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T16:09:41.487810Z","iopub.execute_input":"2025-04-07T16:09:41.488200Z","iopub.status.idle":"2025-04-07T16:09:42.780894Z","shell.execute_reply.started":"2025-04-07T16:09:41.488179Z","shell.execute_reply":"2025-04-07T16:09:42.780172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n# ========================\n# Residual Block\n# ========================\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(channels, channels, 3),\n            nn.InstanceNorm2d(channels),\n            nn.ReLU(inplace=True),\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(channels, channels, 3),\n            nn.InstanceNorm2d(channels)\n        )\n\n    def forward(self, x):\n        return x + self.block(x)\n\n# ========================\n# Generator: ResNet-based\n# ========================\nclass Generator(nn.Module):\n    def __init__(self, input_channels=3, output_channels=3, n_residual_blocks=9):\n        super().__init__()\n\n        model = [\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(input_channels, 64, 7),\n            nn.InstanceNorm2d(64),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n            nn.InstanceNorm2d(128),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n            nn.InstanceNorm2d(256),\n            nn.ReLU(inplace=True),\n        ]\n\n        for _ in range(n_residual_blocks):\n            model.append(ResidualBlock(256))\n\n        model += [\n            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),\n            nn.InstanceNorm2d(128),\n            nn.ReLU(inplace=True),\n\n            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n            nn.InstanceNorm2d(64),\n            nn.ReLU(inplace=True),\n\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(64, output_channels, 7),\n            nn.Tanh()\n        ]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        return self.model(x)\n\n# ========================\n# Discriminator: PatchGAN\n# ========================\nclass Discriminator(nn.Module):\n    def __init__(self, input_channels=3):\n        super().__init__()\n\n        def block(in_channels, out_channels, normalize=True):\n            layers = [nn.Conv2d(in_channels, out_channels, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_channels))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(input_channels, 64, normalize=False),\n            *block(64, 128),\n            *block(128, 256),\n            *block(256, 512),\n            nn.Conv2d(512, 1, 4, padding=1)  # PatchGAN output\n        )\n\n    def forward(self, x):\n        return self.model(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T16:09:42.781866Z","iopub.execute_input":"2025-04-07T16:09:42.782185Z","iopub.status.idle":"2025-04-07T16:09:42.942357Z","shell.execute_reply.started":"2025-04-07T16:09:42.782166Z","shell.execute_reply":"2025-04-07T16:09:42.941227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nos.makedirs(\"checkpoints\", exist_ok=True)  # ✅ Creates the folder if it doesn't exist","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T16:09:42.943551Z","iopub.execute_input":"2025-04-07T16:09:42.943899Z","iopub.status.idle":"2025-04-07T16:09:42.962923Z","shell.execute_reply.started":"2025-04-07T16:09:42.943867Z","shell.execute_reply":"2025-04-07T16:09:42.962051Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport itertools\nimport torchvision.utils as vutils\nimport os\nimport random\n\n# ============ Setup ============\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Set seed\ntorch.manual_seed(42)\nrandom.seed(42)\n\nG_S2R = Generator().to(device)\nG_R2S = Generator().to(device)\nD_R = Discriminator().to(device)\nD_S = Discriminator().to(device)\n\ncriterion_GAN = nn.MSELoss()\ncriterion_cycle = nn.L1Loss()\ncriterion_identity = nn.L1Loss()\n\nlr = 0.0001\nbeta1, beta2 = 0.5, 0.999\noptimizer_G = optim.Adam(itertools.chain(G_S2R.parameters(), G_R2S.parameters()), lr=lr, betas=(beta1, beta2))\noptimizer_D_R = optim.Adam(D_R.parameters(), lr=lr, betas=(beta1, beta2))\noptimizer_D_S = optim.Adam(D_S.parameters(), lr=lr, betas=(beta1, beta2))\n\n# ============ LR Scheduler ============ \nwarmup_epochs = 5\ntotal_epochs = 50\n\ndef lr_lambda(current_epoch):\n    if current_epoch < warmup_epochs:\n        return float(current_epoch + 1) / warmup_epochs  # Warm-up\n    else:\n        return max(0.1, 0.95 ** (current_epoch - warmup_epochs))  # Decay\n\nscheduler_G = optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda)\nscheduler_D_R = optim.lr_scheduler.LambdaLR(optimizer_D_R, lr_lambda)\nscheduler_D_S = optim.lr_scheduler.LambdaLR(optimizer_D_S, lr_lambda)\n\nreal_label_val = 1.0\nfake_label_val = 0.0\n\nlambda_cycle = 10.0\nlambda_id = 5.0\n\n# ============ Fix Input ============\ndef fix_input_shape(img_tensor, name):\n    if img_tensor.dim() == 3:\n        img_tensor = img_tensor.unsqueeze(0)\n    if img_tensor.dim() != 4:\n        raise ValueError(f\"[ERROR] {name} tensor shape invalid: {img_tensor.shape}\")\n    if img_tensor.shape[2] < 7 or img_tensor.shape[3] < 7:\n        raise ValueError(f\"[ERROR] {name} too small for ReflectionPad2d: {img_tensor.shape}\")\n    return img_tensor\n\n# ============ Save Samples ============\ndef save_sample_images(sketch, fake_real, real_image, epoch, output_dir=\"samples\"):\n    os.makedirs(output_dir, exist_ok=True)\n    sample = torch.cat((sketch[0:1], fake_real[0:1], real_image[0:1]), dim=0)\n    vutils.save_image(sample, f\"{output_dir}/epoch_{epoch+1}.png\", nrow=3, normalize=True)\n\n\n# ============ Early Stopping ============\nbest_loss = float('inf')\npatience = 15\nmin_delta = 0.001\npatience_counter = 0\n\n# ============ Training Loop ============\nnum_epochs = 50\nfor epoch in range(num_epochs):\n    epoch_g_loss = 0\n    total_batches = len(train_loader)\n\n    for i, (real_sketch, real_image) in enumerate(train_loader):\n        real_sketch = fix_input_shape(real_sketch.to(device), \"Sketch\")\n        real_image = fix_input_shape(real_image.to(device), \"Real\")\n\n        # Train Generators\n        optimizer_G.zero_grad()\n\n        fake_real = G_S2R(real_sketch)\n        pred_fake = D_R(fake_real)\n        loss_GAN_S2R = criterion_GAN(pred_fake, torch.full_like(pred_fake, real_label_val, device=device))\n\n        fake_sketch = G_R2S(real_image)\n        pred_fake2 = D_S(fake_sketch)\n        loss_GAN_R2S = criterion_GAN(pred_fake2, torch.full_like(pred_fake2, real_label_val, device=device))\n\n        rec_sketch = G_R2S(fake_real)\n        rec_real = G_S2R(fake_sketch)\n        loss_cycle_sketch = criterion_cycle(rec_sketch, real_sketch)\n        loss_cycle_real = criterion_cycle(rec_real, real_image)\n\n        # Identity loss\n        idt_real = G_S2R(real_image)\n        idt_sketch = G_R2S(real_sketch)\n        loss_idt_real = criterion_identity(idt_real, real_image)\n        loss_idt_sketch = criterion_identity(idt_sketch, real_sketch)\n\n        # Total generator loss\n        loss_G = (\n            loss_GAN_S2R + loss_GAN_R2S +\n            lambda_cycle * (loss_cycle_sketch + loss_cycle_real) +\n            lambda_id * (loss_idt_real + loss_idt_sketch)\n        )\n        loss_G.backward()\n        optimizer_G.step()\n\n        # Train Discriminator R\n        optimizer_D_R.zero_grad()\n        loss_real = criterion_GAN(D_R(real_image), torch.full_like(pred_fake, real_label_val, device=device))\n        loss_fake = criterion_GAN(D_R(fake_real.detach()), torch.full_like(pred_fake, fake_label_val, device=device))\n        loss_D_R = (loss_real + loss_fake) * 0.5\n        loss_D_R.backward()\n        optimizer_D_R.step()\n\n        # Train Discriminator S\n        optimizer_D_S.zero_grad()\n        loss_real = criterion_GAN(D_S(real_sketch), torch.full_like(pred_fake2, real_label_val, device=device))\n        loss_fake = criterion_GAN(D_S(fake_sketch.detach()), torch.full_like(pred_fake2, fake_label_val, device=device))\n        loss_D_S = (loss_real + loss_fake) * 0.5\n        loss_D_S.backward()\n        optimizer_D_S.step()\n\n        epoch_g_loss += loss_G.item()\n\n        # Log: First 3 and Last 3 Batches\n        if i < 3 or i >= total_batches - 3:\n            print(f\"[Epoch {epoch+1}/{num_epochs}] [Batch {i+1}/{total_batches}] \"\n                  f\"[D_R: {loss_D_R.item():.4f}] [D_S: {loss_D_S.item():.4f}] [G: {loss_G.item():.4f}]\")\n\n    avg_g_loss = epoch_g_loss / total_batches\n    print(f\"Epoch {epoch+1} Average G Loss: {avg_g_loss:.4f}\")\n\n    # Save sample outputs\n    \n    if (epoch + 1) % 5 == 0:  # Save every 5th epoch\n        save_sample_images(real_sketch, fake_real, real_image, epoch + 1)\n\n\n    scheduler_G.step()\n    scheduler_D_R.step()\n    scheduler_D_S.step()\n\n    # Save best model\n    if best_loss - avg_g_loss > min_delta:\n        best_loss = avg_g_loss\n        patience_counter = 0\n        os.makedirs(\"checkpoints\", exist_ok=True)\n        torch.save(G_S2R.state_dict(), \"checkpoints/G_S2R_best.pth\")\n        torch.save(G_R2S.state_dict(), \"checkpoints/G_R2S_best.pth\")\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1} due to no improvement in generator loss.\")\n            break\n\nprint(\"Training done ✅\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T16:09:42.963789Z","iopub.execute_input":"2025-04-07T16:09:42.964044Z","iopub.status.idle":"2025-04-07T20:59:55.836420Z","shell.execute_reply.started":"2025-04-07T16:09:42.964023Z","shell.execute_reply":"2025-04-07T20:59:55.835159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torchvision.utils as vutils\n\nG_S2R.eval()\nG_R2S.eval()\n\noutput_dir = \"output_images/val_sketch_to_real\"\nos.makedirs(output_dir, exist_ok=True)\n\nwith torch.no_grad():\n    for idx, (val_sketch, _) in enumerate(val_loader):\n        val_sketch = val_sketch.to(device)\n\n        # Generate fake real images from sketch\n        fake_real = G_S2R(val_sketch)\n\n        # Save the output image\n        for i in range(fake_real.size(0)):\n            vutils.save_image(fake_real[i],\n                              os.path.join(output_dir, f\"fake_real_{idx}_{i}.png\"),\n                              normalize=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:59:55.837123Z","iopub.status.idle":"2025-04-07T20:59:55.837510Z","shell.execute_reply":"2025-04-07T20:59:55.837333Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torchvision.transforms as T\n\nG_S2R.eval()\nto_pil = T.ToPILImage()\n\nwith torch.no_grad():\n    for idx, (val_sketch, val_real) in enumerate(val_loader):\n        val_sketch = val_sketch.to(device)\n        val_real = val_real.to(device)\n        fake_real = G_S2R(val_sketch)\n\n        # Display Sketch, Generated, and Ground Truth side-by-side\n        for i in range(fake_real.size(0)):\n            sketch_img = to_pil(val_sketch[i].cpu())\n            gen_img = to_pil(fake_real[i].cpu())\n            real_img = to_pil(val_real[i].cpu())\n\n            fig, axs = plt.subplots(1, 3, figsize=(9, 3))\n            axs[0].imshow(sketch_img)\n            axs[0].set_title(\"Input Sketch\")\n            axs[0].axis(\"off\")\n\n            axs[1].imshow(gen_img)\n            axs[1].set_title(\"Generated Image\")\n            axs[1].axis(\"off\")\n\n            axs[2].imshow(real_img)\n            axs[2].set_title(\"Ground Truth\")\n            axs[2].axis(\"off\")\n\n            plt.tight_layout()\n            plt.show()\n\n        # Show only first few batches (adjust as needed)\n        # if idx >= 2:\n        #     break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T21:02:38.209997Z","iopub.execute_input":"2025-04-07T21:02:38.210334Z","iopub.status.idle":"2025-04-07T21:04:59.717204Z","shell.execute_reply.started":"2025-04-07T21:02:38.210310Z","shell.execute_reply":"2025-04-07T21:04:59.716459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torchvision.utils as vutils\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom zipfile import ZipFile\n\n# Create directory to save output images\noutput_dir = \"generated_samples\"\nos.makedirs(output_dir, exist_ok=True)\n\nG_S2R.eval()  # set generator to eval mode\n\n# Save first N batches (each has batch_size images)\nnum_batches_to_save = 5\n\nwith torch.no_grad():\n    for idx, (sketch, real) in enumerate(val_loader):\n        if idx >= num_batches_to_save:\n            break\n        \n        sketch = sketch.to(device)\n        fake_real = G_S2R(sketch)\n\n        # Loop over batch\n        for i in range(sketch.size(0)):\n            s_img = sketch[i].cpu()\n            f_img = fake_real[i].cpu()\n            r_img = real[i].cpu()\n\n            # Resize for better view in PPT\n            s_img = transforms.ToPILImage()(s_img).resize((128, 128))\n            f_img = transforms.ToPILImage()(f_img).resize((128, 128))\n            r_img = transforms.ToPILImage()(r_img).resize((128, 128))\n\n            # Combine: Sketch | Generated | Real\n            combined = Image.new(\"RGB\", (128 * 3, 128))\n            combined.paste(s_img, (0, 0))\n            combined.paste(f_img, (128, 0))\n            combined.paste(r_img, (256, 0))\n\n            combined.save(f\"{output_dir}/sample_{idx}_{i}.png\")\n\n# ✅ Zip the output folder for download on Kaggle\nzip_path = \"generated_samples.zip\"\nwith ZipFile(zip_path, \"w\") as zipf:\n    for root, _, files in os.walk(output_dir):\n        for file in files:\n            zipf.write(os.path.join(root, file),\n                       arcname=os.path.join(os.path.basename(root), file))\n\nprint(\"✅ Images saved & zipped! You can now download 'generated_samples.zip'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T21:08:24.412400Z","iopub.execute_input":"2025-04-07T21:08:24.412778Z","iopub.status.idle":"2025-04-07T21:08:25.587458Z","shell.execute_reply.started":"2025-04-07T21:08:24.412751Z","shell.execute_reply":"2025-04-07T21:08:25.586678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torchvision.utils as vutils\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms\nfrom PIL import Image\nimport zipfile\n\nG_S2R.eval()\nG_R2S.eval()\n\noutput_dir = \"generated_samples\"\nos.makedirs(output_dir, exist_ok=True)\n\ntransform = transforms.ToPILImage()\n\n# Loop through entire val_loader\nwith torch.no_grad():\n    for idx, (val_sketch, val_real) in enumerate(val_loader):\n        val_sketch = val_sketch.to(device)\n        val_real = val_real.to(device)\n\n        fake_real = G_S2R(val_sketch)\n\n        for i in range(fake_real.size(0)):\n            sketch_img = transform(val_sketch[i].cpu())\n            generated_img = transform(fake_real[i].cpu())\n            real_img = transform(val_real[i].cpu())\n\n            # Combine into one image\n            combined = Image.new(\"RGB\", (sketch_img.width * 3, sketch_img.height))\n            combined.paste(sketch_img, (0, 0))\n            combined.paste(generated_img, (sketch_img.width, 0))\n            combined.paste(real_img, (sketch_img.width * 2, 0))\n\n            # Save combined image\n            combined.save(os.path.join(output_dir, f\"combined_{idx}_{i}.png\"))\n\nprint(\"✅ All images saved!\")\n\n# Zip the folder\nzip_name = \"generated_samples.zip\"\nwith zipfile.ZipFile(zip_name, 'w') as zipf:\n    for root, _, files in os.walk(output_dir):\n        for file in files:\n            zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), output_dir))\n\nprint(f\"📦 Zipped all into {zip_name}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T21:11:18.443572Z","iopub.execute_input":"2025-04-07T21:11:18.444074Z","iopub.status.idle":"2025-04-07T21:11:42.458686Z","shell.execute_reply.started":"2025-04-07T21:11:18.444028Z","shell.execute_reply":"2025-04-07T21:11:42.457579Z"}},"outputs":[],"execution_count":null}]}